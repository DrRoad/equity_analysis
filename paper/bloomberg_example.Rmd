---
title: "Bloomberg Example"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F,
                      warning = F,
                      message = F)

## Set options
options(scipen = 999, # prevent scientific notation on large numbers
        stringsAsFactors = F) # prevent R from turning everything to factors

library(magrittr)
library(tidyverse)
library(lubridate)
library(feather)
working_directory <- here::here()
source(file.path(working_directory,"R/set_paths.R"))
source(file.path(working_directory,"R/data_pipeline_functions.R"))

index_of_interest <- "JALSH"
source_of_interest <- "bloomberg"

```

# NOTES FOR PAPER
How the dataset joins work.

The source defines how to refer to each constituent. Each constituents file has rows of constituent names

1. *constituents -> metadata* : Each metadata file must have a market_identifier field with the exact same constituent name
2. *metadata -> markedata* : Each metadata file defines how to refer to each constituent's marketdata file. The market data datalog file must be the same string as the metadata market_identifier entry
3. *metadata ->  fundamental data* : Each metadata file defines how to refer to each constituent's fundamnetaldata file. The fundamental data datalog file must be the same string as the metadata fundamnetal_identifier entry

# Introduction

In this section we walk through two end-to-end examples of backtesting using this environment. This section should be fully replicable by anybody with access to an x86 computer and the relevant data sources. 

## Enabling replication

This example is fully reproducible. The code to replicate this backtest can be downloaded and run, without modification, from [the Github repository.](https://github.com/riazarbi/euity_analysis_example1)

##  Setting up the environment

Our examples are run on a second generation Intel i-series laptop with 16gb of RAM. The operating system is `Ubuntu 18.04`, but these examples can run on any laptop with `RStudio` or `RStudio Server`. We use `docker` to set up the `RStudio` environment - 

> `docker run -p 8787:8787 -e PASSWORD=complicatedpassword riazarbi/equity_analysis`

This command exposes an RStudio server accessible via web browser at `localhost:8787`. The username is `rstudio` and the password is `complicatedpassword`. After logging in, we click `Project: (None) > New Project > Version Control > Git` in the top right corner of the interface and then enter the repository url before clicking `Create Project` - 

> `https://github.com/riazarbi/euity_analysis_example1`

This will download the files and intialize the project. The project files can be seen in the bottom right panel.

## Example 1: Random portfolios of synthetic data

### Thesis

The investment thesis behind this backtest is that there is an optimal static set of portfolio weights that will result in the maximum possible return. To test this thesis, we create an algorithm that assigns random portfolio weights that it never rebalances. We run this algirothm 150 times, with the intention of selecting the one with the best return.

This thesis is obviously false, but provides a good way to demonstrate the functionality of these scripts.

### Execution

Provided a researcher does not need to make any modifications to the backtester code (which does not diminsh reproducability because changes show up in the git commit logs), there are only two files which need to be configured.


#### `parameters.R` file

The `parameter.R` file looks as follows:

```{r, eval=FALSE, echo=TRUE}
# Define parameters
# Set seed for reproducibility
set.seed(42)

###############################################################
# REPORTING PARAMETERS
annual_risk_free_rate <- 0.04
daily_risk_free_rate <- (1+annual_risk_free_rate)**(1/365.25)-1

##############################################################
# SIMULATED DATA PARAMETERS - ONLY IMPORTANT IF CREATING SIMULATED DATASET
# Index name
index <- "RISK_FREE_GROWERS"
# Universe size
universe_size <- 200
# Average Daily Stock Growth Rate
annual_stock_growth_rate <- annual_risk_free_rate + 0.04
stock_growth_rate <- (1 + annual_stock_growth_rate)**(1/365.25)-1
# Index Coverage of Universe (eg index of 10 stocks on universe of 100 stocks is 1/10)
index_coverage <- 1 # this is how much of the universe is in the index. So 1/4 would have an index of 50 from a universe of 200.
# Timeframe
end_simulation <- Sys.Date() # end dataset creation today
# start backtest
start_simulation <- end_simulation - 365*50 # first data 50 years ago
# standard deviation of daily price jumps (percentage)
price_jump_stddev <- 0.02
# standard deviation of quarterly variation in earnings yield from long run mean (percentage)
earnings_yield_stddev <- 0.2
# standard deviation of quarterly variation in payout ratio from long run mean (percentage)
payout_ratio_stddev <- 0.1

#################################################################
# TRADING PARAMETERS

# Mode - either LIVE or BACKTEST
run_mode <- "BACKTEST"
# Heartbeat duration: how long between heartbeats (seconds)
# I chose 12 hours - twice a day
heartbeat_duration <- 60*60*12
# rebalancing period: how long between portfolio rebalancing (seconds)
rebalancing_periodicity <- 60*60*24*30*10^6 # the extra 10^6 ensures never rebalance

# Universe
constituent_index <- "RISK_FREE_GROWERS"
data_source <- "simulated"

# Specify price and volume fields
# Simulated
price_related_data <- c("date", "open", "high", "low", "close", "last")
volume_data <- c("date", "volume")
market_metrics <- c()
fundamental_metrics <- c() 

# Timeframe
start_backtest <- "2010-01-01" # inclusive
end_backtest <- "2015-12-01" # not inclusive

# Portfolio characteristics
portfolio_starting_configs <- c("CASH", "STOCK")
portfolio_starting_config <- "CASH"
portfolio_starting_value <- 1000000
cash_buffer_percentage <- 0.02 # decimal form
#cash_yearly_compounding_rate <- 0.05 # decimal form not implemented yet

# Trading characteristics
commission_rate <- 0.001
minimum_commission <- 0
standard_spread <- 0.002
soft_rebalancing_constraint <- 0.01 # don't trade if this close to perfect balance

##############################################################
# Process parameters
# * DON'T MODIFY * #
allowed_modes <- c("LIVE", "BACKTEST")
# Convert to date format
library(lubridate)
start_backtest <- ymd(start_backtest)
end_backtest <- ymd(end_backtest)

# Check parameter health
if(!(run_mode %in% allowed_modes)) {
  stop("Set a correct mode in parameters.R: Either LIVE or BACKTEST.")
} 
print(paste("Parameters file specifies", run_mode, "mode."))
```


#### `trials` scripts

There is a sample of a random-weighting algorithm at `scripts/trading/sample_trials/random_trial_1.R`. The exact algorithm is - 

```{r, eval=FALSE, echo=TRUE}
# Expects: a runtime ticker dataset. Metrics optional.
compute_weights <- function(algo_data, metrics) {
  algo_start <- Sys.time()
  # 1. CUT THE DATASET DOWN TO SIZE
  # Drop all entries except the latest one
  algo_data <- algo_data %>% map(~filter(.x, date == max(date)))
  # 2. COMPUTE AGGREGATE MEASURE
  number_tickers <- length(algo_data)
  raw_weights <- sample(runif(number_tickers))
  target_weight <- raw_weights/sum(raw_weights)
  # CREATE LIST OF TICKER NAMES
  portfolio_members <- names(algo_data)
  # PAIR EACH TICKER TO ITS WEIGHT
  target_weights <- data.frame(portfolio_members, target_weight)
  target_weights$portfolio_members <- as.character(target_weights$portfolio_members)
  algo_end <- Sys.time()
  print(paste("INFO: Algorithm runtime:", algo_end - algo_start, "seconds."))
  # RETURN TARGET WIEGHTS DATA FRAME
  return(target_weights)
}
```

In order to run many of these we write 150 copies of the algorithm into the `trials/` directory. Because `seed()` is only set once in a backtest run, each of these trials will have different results even though the code is identical.

```{r, eval=FALSE}
dir.create("trials")

sample <- "scripts/trading/sample_trials/random_trial_1.R"

for (i in seq_along(1:100)) {
  file.copy(sample, paste("trials/random_trial_",
                          i+1,
                          ".R", 
                          sep=""))
}
```


### Results



## Bloomberg Example

# QUERYING DATA

## software prep
5. rstudio - off a flash maybe if no auth. Specify requirements - tidyverse and RBlpapi
1. copy repo to flash - export
3. open bloomberg
4. open bbcomm
## Actual query
1. Open bloomberg_to_datalog.R and make sure parameters are right. We set JALSH / TOP40, 120 months.
2. Close. 
3. source("scripts/1_query_source.R")
4. Select bloomberg to datalog, let it run
 - note: chunking: implicit 25 field limit. 
 - large ticker query limit: limit to 100
 - split fundamnetal and market data - ISIN vs ticker
9. we ran JALSH and TOP40 for 120 months - show screenshot. 
Time: 30 min

# PROCESSING DATA
1. source("scripts/2_process_data.R")
non-interactive to speed things up. Why not part of above? Well you can by calling 0_all.R but maybe you don
't want to because you don't have enought time on that PC.
Time: 12s + 1.8s + .17s + 3.7min

# EDA ON DATA
1. Count files of each category in datalog and make sure they all made it to dataset.
```{r}
# load datalog
datalog <- convert_datalog_to_dataframe() 
# number of files per extension
datalog %>% group_by(ext) %>% summarise(n())
# select just feather
datalog <- datalog %>% filter(ext == "feather")
# get a list of all data types
unique(datalog$data_type)

# DATALOG COUNTS
# number of constituent files in datalog
datalog_constituent_files <- datalog %>% filter(data_type=="constituent_list")
nrow(datalog_constituent_files)
# number of metadata files in datalog
datalog_metadata_files <- datalog %>% filter(data_type=="metadata_array")
nrow(datalog_metadata_files)
# number of tickers in marketdata logs
datalog_unique_market_tickers <- datalog %>% filter(data_type == "ticker_market_data") %>% select(data_label) %>% unique()
nrow(datalog_unique_market_tickers)
# compare to number in datasets
dataset_market_tickers <- list.files(file.path(dataset_directory, "ticker_market_data"))
length(dataset_market_tickers)

# number of tickers in fundamental data logs
# note - not necessarily equal; if ISIN covers two tickers.
datalog_unique_fundamental_tickers <- datalog %>% filter(data_type == "ticker_fundamental_data") %>% select(data_label) %>% unique()
nrow(datalog_unique_fundamental_tickers)
# compare to number in datasets
dataset_fundamental_tickers <- list.files(file.path(dataset_directory, "ticker_fundamental_data"))
length(dataset_fundamental_tickers)
```


# TICKER COUNTS - EXPLORING DATASET DIRECTORY
```{r}

# Strategy - use datasets.
# Filter to just index and source of interest
constituent_tickers <- read_feather(file.path(dataset_directory, "constituent_list", "constituent_list.feather")) %>% 
  filter(source == source_of_interest) %>%  
  filter(index == index_of_interest) %>%
  select(ticker) %>%
  unique() %>% 
  mutate(ticker = paste(ticker, "Equity"))
# number of tickers in constituent lists
nrow(constituent_tickers)

metadata_dataset <- read_feather(file.path(dataset_directory, "metadata_array", "metadata_array.feather"))

metadata_tickers <- metadata_dataset %>% 
  select(market_identifier) %>%
  unique()
# number of tickers in metadata list
nrow(metadata_tickers)

# QUESTION: Does every constituent have metadata?
# Count differences between constituent lists and tickers
setdiff(constituent_tickers, metadata_tickers) # elements in constituent tickers but not in metadata
# note - metadata doesn't have awareness of indexes.

# QUESTION: Does every constituent have market data?
market_datasets <- tools::file_path_sans_ext(dataset_market_tickers) %>% 
  gsub("_", " ", ., fixed = TRUE) %>% 
  enframe()
constituents_without_marketdata <- setdiff(constituent_tickers$ticker, market_datasets$value) # 0 means complete coverage
constituents_without_marketdata

# QUESTION: Does every constituent have fundamental data?
fundamental_datasets <- tools::file_path_sans_ext(dataset_fundamental_tickers) %>% 
  gsub("_", " ", ., fixed = TRUE) %>% 
  enframe() 
# ISINs of constituents
constituent_ISINs <- metadata_dataset %>% filter(source == source_of_interest,
                            market_identifier %in% constituent_tickers$ticker,
                            metric == "fundamental_identifier")
constituents_without_fundamentals <- setdiff(constituent_ISINs$value, fundamental_datasets$value)
constituents_without_fundamentals

constituents_without_fundamentals <- metadata_dataset %>% filter(value %in% constituents_without_fundamentals)
constituents_without_fundamentals <- metadata_dataset %>% filter(market_identifier %in% constituents_without_fundamentals$market_identifier,
                            metric == "LONG_COMP_NAME") %>% select(value)
```

# Per Ticker Quality Check

# Metadata health check

```{r}
incomplete_metadata <- metadata_dataset %>% spread(metric, value) %>% filter(!complete.cases(.)) %>% select_if(~sum(is.na(.)) > 0)

nrow(incomplete_metadata) / nrow(metadata_dataset %>% spread(metric, value))

knitr::kable(colnames(incomplete_metadata), col.names = c("Fields containing NA"))
```
Conclusion: sector models will need to be aware of this.

# Marketdata health check

We need to chekc the health of each ticker's dataset and create a score. Then we can compare across stocks to get a view of our total dataset health.

Things we are concerned with - 

1. Temporal coverage: is there market data for each date the ticker appears in constituent list?
2. Field coverage: is there market data for each date for each field? - can backfill...
3. Distribution: are there any weird outlier values? How many?



```{r, eval=FALSE}
marketdata_tickers_of_interest <- constituent_tickers$ticker %>% gsub(" ", "_", ., fixed = TRUE) %>% paste(.,"feather", sep=".")

read_feather(marketdata_tickers_of_interest[i] %>% file.path(dataset_directory, "ticker_market_data",.))

```


# Fundamental data health check
We need to chekc the health of each ticker's dataset and create a score. Then we can compare across stocks to get a view of our total dataset health



```{r}
dataset_fundamental_tickers

```

## Look ahead bias

Is there already lag? Compare manually but downloading financials

# Manual correctness checks

- Two options here: if two sources, check between them. Or pull yahoo/qhandl and compare that way.