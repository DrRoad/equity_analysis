---
title: "Bloomberg Example"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F,
                      warning = F,
                      message = F)

## Set options
options(scipen = 999, # prevent scientific notation on large numbers
        stringsAsFactors = F) # prevent R from turning everything to factors

library(magrittr)
library(tidyverse)
library(lubridate)
library(feather)
working_directory <- here::here()
source(file.path(working_directory,"R/set_paths.R"))
source(file.path(working_directory,"R/data_pipeline_functions.R"))

index_of_interest <- "JALSH"
source_of_interest <- "bloomberg"

```

# NOTES:
How the dataset joins work.

The source defines how to refer to each constituent. Each constituents file has rows of constituent names

1. *constituents -> metadata* : Each metadata file must have a market_identifier field with the exact same constituent name
2. *metadata -> markedata* : Each metadata file defines how to refer to each constituent's marketdata file. The market data datalog file must be the same string as the metadata market_identifier entry
3. *metadata ->  fundamental data* : Each metadata file defines how to refer to each constituent's fundamnetaldata file. The fundamental data datalog file must be the same string as the metadata fundamnetal_identifier entry



# Setting up the environment
1. Spin up docker container - supply sample code
1. create project from version control - supply ui directions

# QUERYING DATA

## software prep
5. rstudio - off a flash maybe if no auth. Specify requirements - tidyverse and RBlpapi
1. copy repo to flash - export
3. open bloomberg
4. open bbcomm
## Actual query
1. Open bloomberg_to_datalog.R and make sure parameters are right. We set JALSH / TOP40, 120 months.
2. Close. 
3. source("scripts/1_query_source.R")
4. Select bloomberg to datalog, let it run
 - note: chunking: implicit 25 field limit. 
 - large ticker query limit: limit to 100
 - split fundamnetal and market data - ISIN vs ticker
9. we ran JALSH and TOP40 for 120 months - show screenshot. 
Time: 30 min

# PROCESSING DATA
1. source("scripts/2_process_data.R")
non-interactive to speed things up. Why not part of above? Well you can by calling 0_all.R but maybe you don
't want to because you don't have enought time on that PC.
Time: 12s + 1.8s + .17s + 3.7min

# EDA ON DATA
1. Count files of each category in datalog and make sure they all made it to dataset.
```{r}
# load datalog
datalog <- convert_datalog_to_dataframe() 
# number of files per extension
datalog %>% group_by(ext) %>% summarise(n())
# select just feather
datalog <- datalog %>% filter(ext == "feather")
# get a list of all data types
unique(datalog$data_type)

# DATALOG COUNTS
# number of constituent files in datalog
datalog_constituent_files <- datalog %>% filter(data_type=="constituent_list")
nrow(datalog_constituent_files)
# number of metadata files in datalog
datalog_metadata_files <- datalog %>% filter(data_type=="metadata_array")
nrow(datalog_metadata_files)
# number of tickers in marketdata logs
datalog_unique_market_tickers <- datalog %>% filter(data_type == "ticker_market_data") %>% select(data_label) %>% unique()
nrow(datalog_unique_market_tickers)
# compare to number in datasets
dataset_market_tickers <- list.files(file.path(dataset_directory, "ticker_market_data"))
length(dataset_market_tickers)

# number of tickers in fundamental data logs
# note - not necessarily equal; if ISIN covers two tickers.
datalog_unique_fundamental_tickers <- datalog %>% filter(data_type == "ticker_fundamental_data") %>% select(data_label) %>% unique()
nrow(datalog_unique_fundamental_tickers)
# compare to number in datasets
dataset_fundamental_tickers <- list.files(file.path(dataset_directory, "ticker_fundamental_data"))
length(dataset_fundamental_tickers)
```


# TICKER COUNTS - EXPLORING DATASET DIRECTORY
```{r}

# Strategy - use datasets.
# Filter to just index and source of interest
constituent_tickers <- read_feather(file.path(dataset_directory, "constituent_list", "constituent_list.feather")) %>% 
  filter(source == source_of_interest) %>%  
  filter(index == index_of_interest) %>%
  select(ticker) %>%
  unique() %>% 
  mutate(ticker = paste(ticker, "Equity"))
# number of tickers in constituent lists
nrow(constituent_tickers)

metadata_dataset <- read_feather(file.path(dataset_directory, "metadata_array", "metadata_array.feather"))

metadata_tickers <- metadata_dataset %>% 
  select(market_identifier) %>%
  unique()
# number of tickers in metadata list
nrow(metadata_tickers)

# QUESTION: Does every constituent have metadata?
# Count differences between constituent lists and tickers
setdiff(constituent_tickers, metadata_tickers) # elements in constituent tickers but not in metadata
# note - metadata doesn't have awareness of indexes.

# QUESTION: Does every constituent have market data?
market_datasets <- tools::file_path_sans_ext(dataset_market_tickers) %>% 
  gsub("_", " ", ., fixed = TRUE) %>% 
  enframe()
constituents_without_marketdata <- setdiff(constituent_tickers$ticker, market_datasets$value) # 0 means complete coverage
constituents_without_marketdata

# QUESTION: Does every constituent have fundamental data?
fundamental_datasets <- tools::file_path_sans_ext(dataset_fundamental_tickers) %>% 
  gsub("_", " ", ., fixed = TRUE) %>% 
  enframe() 
# ISINs of constituents
constituent_ISINs <- metadata_dataset %>% filter(source == source_of_interest,
                            market_identifier %in% constituent_tickers$ticker,
                            metric == "fundamental_identifier")
constituents_without_fundamentals <- setdiff(constituent_ISINs$value, fundamental_datasets$value)
constituents_without_fundamentals

constituents_without_fundamentals <- metadata_dataset %>% filter(value %in% constituents_without_fundamentals)
constituents_without_fundamentals <- metadata_dataset %>% filter(market_identifier %in% constituents_without_fundamentals$market_identifier,
                            metric == "LONG_COMP_NAME") %>% select(value)
```

# Per Ticker Quality Check

# Metadata health check

```{r}
incomplete_metadata <- metadata_dataset %>% spread(metric, value) %>% filter(!complete.cases(.)) %>% select_if(~sum(is.na(.)) > 0)

nrow(incomplete_metadata) / nrow(metadata_dataset %>% spread(metric, value))

knitr::kable(colnames(incomplete_metadata), col.names = c("Fields containing NA"))
```
Conclusion: sector models will need to be aware of this.

# Marketdata health check

We need to chekc the health of each ticker's dataset and create a score. Then we can compare across stocks to get a view of our total dataset health.

Things we are concerned with - 

1. Temporal coverage: is there market data for each date the ticker appears in constituent list?
2. Field coverage: is there market data for each date for each field? - can backfill...
3. Distribution: are there any weird outlier values? How many?



```{r}
marketdata_tickers_of_interest <- constituent_tickers$ticker %>% gsub(" ", "_", ., fixed = TRUE) %>% paste(.,"feather", sep=".")

read_feather(marketdata_tickers_of_interest[i] %>% file.path(dataset_directory, "ticker_market_data",.))

```


# Fundamental data health check
We need to chekc the health of each ticker's dataset and create a score. Then we can compare across stocks to get a view of our total dataset health



```{r}
dataset_fundamental_tickers

```

## Look ahead bias

Is there already lag? Compare manually but downloading financials

# Manual correctness checks

- Two options here: if two sources, check between them. Or pull yahoo/qhandl and compare that way.