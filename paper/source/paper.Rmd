---
# IMPORTANT: Change settings here, but DO NOT change the spacing. 
# Remove comments and add values where applicable. 
# The descriptions below should be self-explanatory

title: "A Data Scientific Approach to Equity Backtesting"
#subtitle: "Emphasising Reproducibility and Transparency"

documentclass: "elsarticle"

# Comment: ----- Follow this pattern for up to 5 authors
Author1: "Riaz Arbi"  # First Author
Ref1: "University of Cape Town, South Africa" # First Author's Affiliation
#Email1: "nfkatzke\\@gmail.com" # First Author's Email address

#Author2: "John Smith"
#Ref2: "Some other Institution, Cape Town, South Africa"
#Email2: "John Smith\\@gmail.com"
#CommonAffiliation_12: TRUE # If Author 1 and 2 have a common affiliation. Works with _13, _23, etc.

keywords: "Equity Backtesting \\sep Reproducible Research \\sep Event-based Backtesting \\sep R \\sep RStudio" # Use \\sep to separate
#JELCodes: "L250 \\sep L100"

# Comment: ----- Manage headers and footers:
#BottomLFooter: $Title$
#BottomCFooter:
#TopLHeader: \leftmark # Adds section name at topleft. Remove comment to add it.
BottomRFooter: "\\footnotesize Page \\thepage\\" # Add a '#' before this line to remove footer.
addtoprule: TRUE
addfootrule: TRUE               # Use if footers added. Add '#' to remove line.

# Setting page margins:
margin: 2.3 # Sides
bottom: 2 # bottom
top: 2.5 # Top

linenumbers: FALSE # Used when submitting to journal
AddTitle: TRUE # Used when submitting to peer reviewed platform. This will remove author names. 

HardSet_layout: TRUE # Hard-set the spacing of words in your document. This will stop LaTeX squashing text to fit on pages, e.g. This is done by hard-setting the spacing dimensions. Set to FALSE if you want LaTeX to optimize this for your paper. 
bibliography: Tex/ref.bib       # Do not edit: Keep this naming convention and location.
RemovePreprintSubmittedTo: FALSE  # Removes the 'preprint submitted to...' at bottom of titlepage
Journal: "Journal of Finance"   # Journal that the paper will be submitting to, if RemovePreprintSubmittedTo is set to TRUE.
toc: yes                         # Add a table of contents
numbersections: yes             # Should sections (and thus figures and tables) be numbered?
fontsize: 11pt                  # Set fontsize
linestretch: 1.2                # Set distance between lines.
link-citations: TRUE            # This creates dynamic links to the papers in reference list.
output:
  pdf_document:
    keep_tex: TRUE
    template: Tex/TexDefault.txt
    fig_width: 3.5 # Adjust default figure sizes. This can also be done in the chunks of the text.
    fig_height: 3.5
    include:
      in_header: Tex/packages.txt # Reference file with extra packages
abstract: |
  Abstract to be written here. The abstract should not be too long and should provide the reader with a good understanding what you are writing about. Academic papers are not like novels where you keep the reader in suspense. To be effective in getting others to read your paper, be as open and concise about your findings here as possible. Ideally, upon reading your abstract, the reader should feel he / she must read your paper in entirety.
  
---
\pagebreak 


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.width = 6, fig.height = 5, fig.pos="H", fig.pos = 'H')
```


# Introduction \label{Introduction}

Intro intro intro

Last paragraph 

This paper is organised as follows. Section \ref{Backtest section} briefly introduces the practice of backtesting of investment strategies in academia and industry, discusses the replication crisis in the academic literature, and enumerates common methodological issues that compromise the validity of research. Section \ref{Reproducible Research} describes the goals and philosophy of the reproducible research movement and its links to the open source software movement, with a special focus on the R statistical programming language, the Tidy approach to data organization and the use of scripting to facilitate reprducibility and transparency in scientific research. Section \ref{Backtester Documentation} introduces a backtesting template that is roughly compatible with the structure of a research compendium - that is, a basic minimum viable unit of reproducible research.

# Anomalies Research in Theory and Practice\label{Backtest section}

Investigations into why the returns of some common stocks outperform others have a long history. Professional bodies of knowledge, often captured in heuristics or rules of thumb, have existed since at least the early 20th century. @1959Graham collected the body of knowledge of one approach, value investing, into a single textbook in 1934. 

## Academic Anomalies Research

As time has progressed, professionals and academics have sought to test these rules in a rigorous manner. A standard way to test whether a particular heuristic may identify abnormal returns is to investigate whether the rule would have identified an abnormal population in the historical record. This process is known as backtesting. Several foundational papers of investment management (for instance, @Fama1998, @Shiller1998) use backtesting methods as a basis for their findings. 

Unfortunately, backtesting is a very difficult operation to achieve correctly. A backtester has to ensure that their dataset is complete and free of error, which is difficult to know after the fact. For instance, the exact date on which the financial results of a company were released may not be known. End-of-year results are never released on the 31st of December, since those financials need to be computed and audited after the fact but prior to release. The actual release date may be months later, and if the information contained in the financials drive returns, the timing of the impact of the release on the market be simply be unknowable. This problem (which is just an example of many) can be dealt with in a variety of ways, all of which require a judgment call from the researcher. 

It is standard a aspiration for researchers to disclose all the judgment calls made during the backtest process, but these disclosures are invariably incomplete. This makes validation by other researchers very difficult. At the extreme, independent validators may arrive at a completely different conclusion (@Hou2017); with ambiguity on both sides it is unclear whether the discrepancy is due to differences in underlying data, prepearation methodologies, mathematical errors or some other factor unrelated to the question at hand. 

## Typical Academic Backtest

*The Cross-Section of Expected Stock Returns* (@Fama1992) is a seminal paper in anomalies research. An excerpt from the abstract reads - 

> Two easily measured variables, size and book-to-market equity, combine to capture the cross-sectional variation in average stock returns associated with [the] market...

The paper mentions the following methodological details - 

### Dataset Creation
1. Data: the intersection of nonfinancial NYSE, AMEX and NASDAQ returns from the Center for Research in Security Prices for price returns and Compustat income statement and balance sheet data for fundamentals.
2. Cleaning: Only update fundamental data in the calendar year >6 months after the valid date of the filing. By doing this, they impose at least a 6 month lag between annual filing valid dates and introduction to the dataset.
3. Rebalancing: market equity at the end of December is used to calculate book-to-market, leverage and price to earnings ratios. Market equity for June is used to calculate size. The offset is to prevent general market timing from affecting results.
4. In order to be able to calculate $\beta$, stocks are only included if there are monthly returns available for at least 24 of the last 60 months preceding July of the year being computed.

### Portfolio Creation

They sort all NYSE stocks by market equity size, and compute decile breakpoints. All NYSE, AMEX and NASDAQ shares are then allocated to those breakpoints. NYSE breakpoints are used to avoid dominance by NASDAQ small caps across the lower deciles. 

Within each decile, they calculate the $\beta$ values for all NYSE stocks, and further subdivide each market equity decile cohort into $\beta$-base deciles. This is done because size and $\beta$s of size portfolios are highly correlated, so test conflate the size effect with the $\beta$ effect.

The end result is 100 portfolios, within which stocks are equally weighted.

## The Replication Crisis

The academic discipline of anomalies research (as this area is now known) has grown into a substantial corpus of contradictory claims. These contradictions are mirrored in the business of investment management by competing investment philosopies, each with its own library of peer-reviewed papers (@Damodaran2012). While markets may very well accommodate contradictory drivers of returns it is often difficult to discern whether differing claims are the result of geniune facts or the result of methodological differences.

@Hou2017 attempt to replicate the entire anomalies literature to identify with results can actually be confirmed. They attempt to replicate 447 anomalies and find that between 64% and 85% of them are insignificant. In other words, they suspect widespread misuse of statistical analysis to  make claims that are not, in fact, true. This is possible, in part, because practitioners have a large degree of discretion in determining every aspect of the backtesting analysis chain.@Hou2017 attempt to set out a common set of replicaiton procedures to standardise research output, including (but not limited to):

- Specifying datasets: Compustat Annual and Quarterly Fundamental Files; Center for Research and Security Prices.
- Specifying breakpoints: Use NYSE breakpoints
- Use value-weighted, not equal-weighted portfolios
- No sample screening (ie don't exclude stocks because of some arbitrary cutoff)
- For annually-composed portfolios, resort at the end of June
- Incorporate fundamental data four months after valid date

Many of the above guidleines are designed to avoid outsize effects from micro capitalization shares, which form 3% of market value but comprise 60% of the total number of stocks. These stocks often suffer from high transaction costs and low liquidity, which muddy the waters when testing for a particular factor.

## Common Biases and Methodological Errors



- Poor documentaiton of data collection and munging
- Ambiguity around actual operations done to organise, clean and drop data
- No verifiability of error
- Unable to detect p-hacking
- Biases: look-ahead, survivorship, data mining
- No transparency (replication, verifiability)
- Backtest overfitting - de Prado, Bailey et al; implicit data mining

## Commercial and Open Source Backtesting Software Packages

- Survey: Proprietary, free web, free open source
- Prop are out because not transparent
- Open Source: See zipline, qstrader, 
- Poor data provenance (yahoo)
- Absence of fundamental data
- Built for pairs and single stocks, not portfolios
- OOP makes transparency difficult (black box)
- Presupposes good data hygeine & pre-munging



#  Reproducible Research and The R Programming Language\label{Reproducible Research}

In recent years, as computational power and storage has become increasingly cheap and available, there have been persistent calls for a review of the way that scientific reseach is packaged and presented (@Stodden2013, @Koenker2009). 

Although scientific research is an accretive process, involving verification and building upon earlier datasets and findings, the computational work done by researchers is often poorly documented, or absent (@Stodden2013). This introduces the risk that views or principles that are considered to be scientifically verified may, in fact, be false.

Gaps in documentation allow bad reasoning, calculation errors or spurious results to creep in to the corpus of knowledge of a discipline because peers are forced to take it on faith that a researcher has a proper understanding of underlying mathematical concepts, and that workings have been thouroughly cross-checked before release.

Worse still, because follow-up replication is difficult to impossible with poor documentation or data availability, incorrect beliefs can persist for years, or even decades (@Munafo2017). *P-hacking*, or the selection of data and finessing of results to conform to significance tests of 5% or lower, has resulted in a situation where a research finding is as likely to to be false it is to be true (@Ioannidis2005). 

## Reproducible Research Terminology

In response to these critiques, a taxonomy of reproducibility has emerged, allowing peers to effectively categorize research.

This taxonomy allows us to dinstinguish reveiwability (which assumes mathematical competence and focuses on reasoning) from reproducibility (which allow the extension of the review into assessement of mathematical competence and quality of data).

| Term  | Description |
| :------------ | :--------------------------------------------- | 
| Confirmable   | Main conclusions can be attained independently  |
| Reviewable    | Descriptions of methods can be independently assessed and judged | 
| Replicable    | Tools are made available that would allow duplication of results  | 
| Auditable     | Sufficient records exist (perhaps privately) to defend research| 
| Reproducable | Auditable research is made openly available, including code and data |
| To Verify | To check that computer code correctly performs the intended operation |
| To Validate | To check that the results of a computation agree with observations |

Table: Common Terminology in Reproducible Science (@Stodden2013)\label{Table1}

This paper uses reproducibility in the sense defined in \ref{Table1}, which is to say, it refers to research output being bundled and distributed with well documented, fully transparent code and data that allows a peer to fully review, replicate, audit and thereby confirm results of a body of work (@Stodden2013).

## Distinguising Between Open Source ands Closed Source Software

In general, software is written in text files, by humans, in a particular programming language. This *source code* is *compiled* or *interpreted* by another program into binary machine code (which is not readable by humans) and distributed for execution. The software for the popular spreasheet program Microsoft Excel, for instance, would be written by a group of humans in a programming language. These text files would be compiled by a compiler program into a binary executable program, which is distributed to consumers who can execute the binary. When someone clicks the Excel icon on their computer to 'launch' the program, they are executing the binary. 

Open source software is software for which the *source code* is made publically available. Microsoft Excel is closed source, and the source code is not made available. Source code can be read by humans, binary cannot. Changes to source code can be meaningfully interpreted by humans, because one can read the changes in plain text. Changes to binary code cannot be interpreted by humans, because the changes are simply alterations to a very long sequence of `0` and `1` digits. It is trivial to compile source code into binary code. It is extremely difficult to accurately decompile binary code into source code, and the tools are not readilty available (@Li2004).

## The Relationship between Reproducible Research and Open Source Software

Open source software development principles exhibit properties which are useful to practitioners of reproducible research. 

By definition, the text-based nature of source code makes all source code reproducible (and, by corollary, auditable). The implications of this are twofold. Firstly, a researcher using open source software for computation can be assured that the full software stack will be open to scrutiny. That is, at the limit, an auditor could inspect every single line of code from some arbitrary 'open starting point' up to verify that the computations are correct. This 'open starting point' is as the operating systme level for linux based machines, and from the application level up for Mac and Windows based machines.

Secondly, and perhaps more importantly, the open source software community has decades of experience in defining good practices for creating and maintaining large, open, verfiable and repeatable enviroments. These principles are often quoted in the folkloric 'Unix Way', which manifests in epithets such as "text is the universal interface" (@Baum2002), "each unit should do one thing and do it well",  and "build things to talk to other things" (@Raymond2003). Reproducible research practice is built on the same principles - simple, readable text-based data and code where possible; clear separation between data, code and results; complete transparency coverage; and portability by design (see @Baum2002, @Bache2014, @Wickham2014).

Historically, there has been close collaboration between the open source software and the free software communities. This means that, as a rule, most open source software is free or has a free analogue. The core tools of Data Science, for instance (such as the python and R programming languages, the Apache Foundation of big data processing packages and the Jupyter and RStudio interactive development environemnts), are all open source and free. This means that the *cost* of replication is not prohibitive: in general, reproducible research should be reproducible for free on commodity hardware.

## The Suitability of R for Reproducible Research

The R programming language was originally conceived as a project to build out an open source statistical programming environment. In development since 1997, base R is now a mature ecosystem comprising a scriptable language, graphical rendering system and debugger (@RCoreTeam2018). A large community of third party developers (some commercial, most free) have extended base R with over 13000 packages, including a fully-fledged interactive development environmnent (Rstudio), interactive dashboarding web server (Shiny) and bindings to LaTex for simple rendering of publication-ready LaTex documents from markdown (knitr and rmarkdown) (@RStudioTeam2015). Because of its roots in open source, it adheres to many open source software conventions. For instance- 

- All source code is freely available
- All development work in R is done in plain text files, which are transparently human and machine readable.
- The R project has adopted the GNU General Public License verison 2, which "does not restrict anyone from making use of the program in a specific field of endeavor" (@RCoreTeam2018). This free avaialability means that anybody with a commodity computer and an internet connection can install and use R.

Although the reproducible research community is principles-based and language-agnostic, the free, text-based nature of the R language have made it a good candidate for reproducible work (@Gentleman2004). @Marwick2018 review the concept of a *research compendium*, and explore how research done in R can be packaged into compendia. They argue that a compendium is defined by three principles - 
1. Files should be organised according to a prevailing standard so that others can immediately understand the structure of the project, and pipe the compendium to environments that expect the standard structure with no structural modifications.
2. Maintain a clear separation between data, method and output. Separation means that data is treated as read only, and that all steps in getting from source data to output is documented. Output should be treated as disposable, and rebuildable from the programmatic application of the method to the data.
3. Specify the computational environment that was used to conduct the research. This provides critical information to a replicator on the undelrying tooling needed to support the analysis.

These principles are intended to ensure that, given a certain input (the data) and a certain operation (the method), a deterministic output results (the analysis). With these details out of the way, an auditor can focus on verifying the correctness of the reasoning (i.e should we use this theory) and of the implementation (i.e are there any errors in the math).

The richness of the R ecosystem means that one can complete the full research life cycle from data loading to publishing without leaving the R ecosystem (@Baumer2014). Data can be loaded, cleaned, transformed, modeled and aggregated in R. The results can be written up in RStudio - in fact, the code and write-up can be combined into a single text-only document using the RMarkdown format - and exported to a wide range of academically accepted typsets and formats (such as Tex, Microsoft Word, Markdown and HTML). Because R is scriptable, R scripts can also be used in production environments, where results can be periodically recomputed and handed on to some other production process.

This property dramatically simplifies the complexity of building a compendium and the required skill-set of a replicator. For instance, a single-platform research compendium can be assessed by anyone with knowledge of that single environment. Multi-platform compendia need replicators well-versed in each platform, which dramatically reduces the pool of potential replicators. 

Upon publication, code versioning tools such as Git allow the public to view all future changes to the code or data transparently, and, if code changes over time, to assess the impact of those changes on the result.

## Tidy Extensions to Base R

The open source and reproducible science principles of interoperability, readibility, modular code and common standards have been extended into the prcatice of data manipulation by @Wickham2014. The collection of R packages which fall under this banner are collectively known as the 'Tidyverse'. These packages provide a common, idiosyncratic syntax for data ingestion, munging, manipulation and visualization that are opinionated in their expectations about data structure and focused on enabling readable code (@Wickham2017). According to Wickham, tidy data intuitively maps the meaning of a dataset to its structure - 

> 1. Each variable forms a column. 
> 2. Each observation forms a row. 
> 3. Each type of observational unit forms a table. 

Whether there is a single optimal data structure is up for debate. What is not up for debate is that that the adoption of a common standard for structures allows researchers to make assumptions about the meaning of data before engaging with the content, i.e simply knowing that data is 'tidy' means that a researcher knows that each column maps to a variable, and each row an observation, *prior to looking at the data*. Setting of standards enables the decoupling of processes - process (a) can produce some peice of analytical output *with no knowledge of what it will be used for*. As long as it is 'tidy', and process can consume that output on the understanding that is confirms to the 'tidy' standard and take the analysis further. In this way, the 'tidy data' concept push the Unix Way concept of 'build things that talk to other things' into the practice of structuring data for reproducible research.

## The Potential for Reproducible Research to Address The Replication Crisis in Anomalies Research

- An argument that these approaches and tools will improve backtesting, both for academics and for practitioners
- A proviso that none of this replaces critical thinking or good data hygeine. Still need to get data, scrutinize it, make judgment calls etc etc.
- Reference the definition of a compendium


# A Reference Implementation of a Data Scientific Equity Backtester in R\label{Backtester Documentation}

The problem of proprietary data, the use of the log, and the transformation into tidy data

Restructure code along the ines of a compendium
Packaging Data analytical Work Reproducibly Using R (and Friends)

## My documentation goes here
## Features

- data versioning
- multi-source
- multi-index
- code version control
- clear procedural workflow
- event-driven
- tidy data
- pbo
- host of metrics
- bias avoidance

# Conclusion\label{conclusion}

This paper has explored the various ways in which the lack of transparency in anomalies research makes it difficult to discern spurious results from geniune findings. This 'replication crisis' has strong analogues in other academic disciplines. We have argued that the 'reproducible science' response to this crisis in science at large has potential to address much of the issues that bedevil anomalies replication. 

We have introduced a collection of R scripts, organised into a compendium, that can be used to conduct anomalies research in a transparent and reproducible way. These scripts utilize only free, open source software and to organize data along the lines of 'tidy' data. Using plain text computer code to collect, process, structure and analyse data represents a good approach to producing research that is easy to reproduce. 

We avoid the problem of proprietary data distribution by including customizable data query scripts. These scripts query proprietary vendors and save the results in a standard way. Bundling these query scripts in a compendium enables a replicator to rebuild the dataset programatically and non-interactively from source.

This collection of scripts make it possible to test an investment algorithm against an index of stocks, where each stock comprises a set of daily observations of price data plus an arbitrary number of attributes. The scripts use the event-based backtest method (as opposed to vectorized methods) which make it easy to avoid look-ahead bias and to introduce non-standard data to the algorithm. Transaction cost and slippage modelling, while rudimentary, exist and can be refined.

Using this code base as a starting point should save a research a great deal of time in preparing stock data for backtesting, and the open source nature of the project ensures that any researcher can comb the operations for bugs or implement features that are not present. While it is not expected that this code base is necessary or sufficient for end-to-end backtesting, it represents a solid base for ongoing development. 

@Katzke2017

<!-- Make title of bibliography here: -->
<!-- \newpage -->

# References
